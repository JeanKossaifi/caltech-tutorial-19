{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Regression Networks with ``TensorLy`` and ``PyTorch`` as a backend\n",
    "\n",
    "In this notebook, we will show how to combine TensorLy and PyTorch in to implement the tensor regression layer, as defined in **Tensor Regression Networks**, _Jean Kossaifi, Zachary C. Lipton, Aran Khanna, Tommaso Furlanello and Anima Anandkumar_, [ArXiV pre-publication](https://arxiv.org/abs/1707.08308).\n",
    "\n",
    "\n",
    "Specifically, we use [TensorLy](http://tensorly.org/dev/index.html) for the tensor operations, with the [PyTorch](http://pytorch.org/) backend.\n",
    "You will need PyTorch version '0.4.0' or later to run this code.\n",
    "\n",
    "We demonstrate the method on the MNIST dataset, which consists of images of digits between 0 and 9 (60,000 images for for training and 10,000 for testing). The task is to predict, given an image, which digit it represents.\n",
    "\n",
    "# Tensor Regression Layers\n",
    "\n",
    "Traditional Deep Convolutional Neural Networks consist of a series of convolutions, non-linearities and pooling, that result in an activation tensor. That activation tensor is typically passed on to a flattening layer and one or several fully-connected layers generate the outputs. \n",
    "\n",
    "For instance, a typical architecture would be defined, in PyTorch as follows:\n",
    "\n",
    "```python\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(512, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "```\n",
    "\n",
    "The idea of tensor regression layers is to instead leverage the spatial structure in the activation tensor\n",
    "and formulate the output as lying in a low-rank subspace that jointly models the input and the output. This is done by means of a low-rank tensor regression.\n",
    "\n",
    "The architecture corresponding to the previous one would be similar except that the Flatten layer and all fully-connected layers have been replaced by a Tensor Regression Layer.\n",
    "\n",
    "In code, this would look something like this:\n",
    "\n",
    "```python\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.trl = TRL(ranks=(10, 3, 3, 10), input_size=(batch_size, 50, 4, 4), output_size=(batch_size,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = self.trl(x)\n",
    "        return F.log_softmax(x)\n",
    "```\n",
    "\n",
    "In this notebook, we will demonstrate how to implement easily the TRL using TensorLy and MXNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with TensorLy and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import inner\n",
    "from tensorly.random import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import TensorLy and set the backend to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "device = 'cpu'\n",
    "# to run on GPU, uncomment the following line:\n",
    "#device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the MNIST dataset. The following code will automatically download the data in a `data` folder if it hasn't already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/datasets/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('~/datasets/', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Tensor Regression Layer as a PyTorch layer\n",
    "\n",
    "We wrap the code needed to perform low-rank tensor regression into a layer so we can connect it to a deep convolutional net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRL(nn.Module):\n",
    "    def __init__(self, input_size, ranks, output_size, verbose=1, **kwargs):\n",
    "        super(TRL, self).__init__(**kwargs)\n",
    "        self.ranks = list(ranks)\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if isinstance(output_size, int):\n",
    "            self.input_size = [input_size]\n",
    "        else:\n",
    "            self.input_size = list(input_size)\n",
    "            \n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = [output_size]\n",
    "        else:\n",
    "            self.output_size = list(output_size)\n",
    "            \n",
    "        self.n_outputs = int(np.prod(output_size[1:]))\n",
    "        \n",
    "        # Core of the regression tensor weights\n",
    "        self.core = nn.Parameter(tl.zeros(self.ranks), requires_grad=True)\n",
    "        self.bias = nn.Parameter(tl.zeros(1), requires_grad=True)\n",
    "        weight_size = list(self.input_size[1:]) + list(self.output_size[1:])\n",
    "        \n",
    "        # Add and register the factors\n",
    "        self.factors = []\n",
    "        for index, (in_size, rank) in enumerate(zip(weight_size, ranks)):\n",
    "            self.factors.append(nn.Parameter(tl.zeros((in_size, rank)), requires_grad=True))\n",
    "            self.register_parameter('factor_{}'.format(index), self.factors[index])\n",
    "        \n",
    "        self.core.data.uniform_(-0.1, 0.1)\n",
    "        for f in self.factors:\n",
    "            f.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Combinne the core, factors and bias, with input x to produce the (1D) output\n",
    "        \"\"\"\n",
    "        pass \n",
    "    \n",
    "    def penalty(self, order=2):\n",
    "        \"\"\"Add l2 regularization on the core and the factors\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network will be composed of a series of two convolutions, pooling, non-linearity.\n",
    "Instead of using a flattening layer followed by a fully-connected layer, we will use our newly defined low-rank Tensor Regression Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        self.trl = TRL(ranks=(10, 3, 3, 10), input_size=(batch_size, 50, 4, 4), output_size=(batch_size,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = self.trl(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously, we define the optimiser to be an SGD an define the criterion to optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop. Notice the penalty on the regression weights added to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 5 # Number of epochs\n",
    "regularizer = 0.001\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train(n_epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Important: do not forget to reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output,target) + regularizer*model.trl.penalty(2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss = criterion(output,target)\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('mean: {}'.format(test_loss))\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "       100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
